#!/bin/bash

#SBATCH --gres=gpu:a100:1 --nodelist=g128
#SBATCH -p long
#SBATCH -J try_
#SBATCH --cpus-per-task=10
#SBATCH --mem=30000
#SBATCH -t 9-23:59:59

source /home/cosuji/anaconda3/etc/profile.d/conda.sh
conda activate ntorch

## Declare your paths here
dataset="/home/cosuji/spinning-storage/cosuji/NLG_Exp/gem/data/deepnlg"
write_path="/home/cosuji/spinning-storage/cosuji/NLG_Exp/gem/pipe_results"
model_name="t5-large"
map_model="t5"
lr=1e-5
tbsize=16
task="ordering"

# Edit this jobs for each model and task
# Training the model
#for task in ordering structuring lexicalization reg; do
python3 ../main.py --tokenizer "$model_name" \
                        --model_path "$model_name" \
                        --task "$task" \
                        --data_path "$dataset" \
                        --epochs 20 \
                        --learning_rate $lr \
                        --train_batch_size $tbsize \
                        --early_stop 10 \
                        --max_length 300 \
                        --write_path "$write_path" \
                        --verbose \
                        --batch_status 8
#done

# echo "$task generating inferences!"
# python3 ../inference.py --tokenizer "$model_name" \
#                 --model_path "$write_path" \
#                 --task "$task" \
#                 --data_path "$dataset" \
#                 --batch_size 16 \
#                 --max_length 300 \
#                 --verbose \
#                 --batch_status 

# # Mapping the outputs
# echo "$task mapping!"
# python3 ../mapping.py --previous_data "$dataset" \
#                 --pipeline_data "$write_path" \
#                 --previous_task "$task" \
#                 --Gen_model "$map_mode

# Surface Realization
# echo "Surface Realization!"
# python3 ../realization.py --model "$map_model" \
#                     --task "$task" \
#                     --surface_path "$surfacevocab" \
#                     --write_path "$write_pat


